# üóÑÔ∏è PostgreSQL Migration Guide - PolyWatch Alerts

**Complete guide to migrate from file-based storage to PostgreSQL**

---

## üèÜ Best Free PostgreSQL Services (2025)

### Comparison Table

| Service | Free Storage | Free Compute | Connections | Best For | Limitations |
|---------|-------------|--------------|-------------|----------|-------------|
| **Supabase** | 500 MB | Shared CPU, 500 MB RAM | Unlimited | Full-featured, easy setup | Pauses after 7 days inactivity |
| **Neon** | 3 GB | 0.5 vCPU shared | 100 | Best free tier, serverless | 5 GB total storage limit |
| **Railway** | 1 GB | Shared | 100 | Same platform as bot | Uses your $5 credit |
| **Render** | 1 GB | Shared | 100 | Free for 90 days | Then $7/month |

### ü•á **RECOMMENDED: Supabase**

**Why Supabase is Best:**
- ‚úÖ **Completely FREE forever** (not trial)
- ‚úÖ **500 MB database** (enough for 10,000+ users)
- ‚úÖ **Built-in auth, storage, realtime** (bonus features)
- ‚úÖ **Excellent dashboard** (easy to manage)
- ‚úÖ **No credit card required**
- ‚úÖ **Generous free tier** (50k monthly active users)
- ‚úÖ **Auto-backups** included

**Only Limitation:**
- ‚ö†Ô∏è Pauses after 7 days of inactivity (but your bot will keep it active!)

---

## üìã What You'll Need

Before starting:
- [ ] Supabase account (free)
- [ ] Your current `data/db.json` file (for migration)
- [ ] 30 minutes of time
- [ ] Basic SQL knowledge (I'll provide all queries)

---

## üöÄ Step-by-Step Migration Plan

### Phase 1: Setup Supabase (10 minutes)
### Phase 2: Create Database Schema (5 minutes)
### Phase 3: Update Bot Code (10 minutes)
### Phase 4: Migrate Existing Data (5 minutes)
### Phase 5: Test & Deploy (5 minutes)

---

## üìù PHASE 1: Setup Supabase

### Step 1.1: Create Supabase Account

1. **Go to:** https://supabase.com
2. **Click:** "Start your project"
3. **Sign up with GitHub** (recommended)
4. **Verify email** if prompted

### Step 1.2: Create New Project

1. **Click:** "New Project"
2. **Fill in details:**
   ```
   Name: polywatchalerts
   Database Password: [Generate strong password - SAVE THIS!]
   Region: Choose closest to you (e.g., US East, EU West)
   Pricing Plan: Free
   ```
3. **Click:** "Create new project"
4. **Wait 2-3 minutes** for project to initialize

### Step 1.3: Get Connection Details

1. **Go to:** Project Settings (gear icon) ‚Üí Database
2. **Copy these values:**
   ```
   Host: db.xxxxxxxxxxxxx.supabase.co
   Database name: postgres
   Port: 5432
   User: postgres
   Password: [your password from step 1.2]
   ```
3. **Also copy:** Connection string (URI format)
   ```
   postgresql://postgres:[YOUR-PASSWORD]@db.xxxxxxxxxxxxx.supabase.co:5432/postgres
   ```

---

## üìù PHASE 2: Create Database Schema

### Step 2.1: Open SQL Editor

1. **In Supabase dashboard:** Click "SQL Editor" (left sidebar)
2. **Click:** "New query"

### Step 2.2: Create Tables

**Copy and paste this SQL, then click "Run":**

```sql
-- Users table
CREATE TABLE users (
  tg_id BIGINT PRIMARY KEY,
  username TEXT,
  first_name TEXT,
  referred_by BIGINT,
  created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Watchers table (user watching addresses)
CREATE TABLE watchers (
  id SERIAL PRIMARY KEY,
  tg_id BIGINT NOT NULL,
  address TEXT NOT NULL,
  added_at TIMESTAMPTZ DEFAULT NOW(),
  UNIQUE(tg_id, address)
);

-- User settings table
CREATE TABLE user_settings (
  tg_id BIGINT PRIMARY KEY,
  min_dm_usd INTEGER,
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- Cursors table (tracking last seen trades per address)
CREATE TABLE cursors (
  address TEXT PRIMARY KEY,
  last_ts BIGINT,
  last_tx TEXT,
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- DM transaction deduplication (48h TTL)
CREATE TABLE dm_tx_seen (
  tg_id BIGINT NOT NULL,
  address TEXT NOT NULL,
  tx_hash TEXT NOT NULL,
  seen_at TIMESTAMPTZ DEFAULT NOW(),
  PRIMARY KEY (tg_id, address, tx_hash)
);

-- Channel transaction deduplication (7 day TTL)
CREATE TABLE channel_tx_posted (
  tx_hash TEXT PRIMARY KEY,
  posted_at TIMESTAMPTZ DEFAULT NOW()
);

-- Create indexes for performance
CREATE INDEX idx_watchers_tg_id ON watchers(tg_id);
CREATE INDEX idx_watchers_address ON watchers(address);
CREATE INDEX idx_dm_tx_seen_at ON dm_tx_seen(seen_at);
CREATE INDEX idx_channel_tx_posted_at ON channel_tx_posted(posted_at);
CREATE INDEX idx_users_referred_by ON users(referred_by);

-- Auto-cleanup old deduplication records
CREATE OR REPLACE FUNCTION cleanup_old_dm_tx()
RETURNS void AS $$
BEGIN
  DELETE FROM dm_tx_seen WHERE seen_at < NOW() - INTERVAL '48 hours';
END;
$$ LANGUAGE plpgsql;

CREATE OR REPLACE FUNCTION cleanup_old_channel_tx()
RETURNS void AS $$
BEGIN
  DELETE FROM channel_tx_posted WHERE posted_at < NOW() - INTERVAL '7 days';
END;
$$ LANGUAGE plpgsql;
```

### Step 2.3: Verify Tables Created

1. **Click:** "Table Editor" (left sidebar)
2. **You should see:** 6 tables (users, watchers, user_settings, cursors, dm_tx_seen, channel_tx_posted)

---

## üìù PHASE 3: Update Bot Code

### Step 3.1: Install PostgreSQL Driver

**In your project directory, run:**

```bash
npm install pg
npm install --save-dev @types/pg
```

### Step 3.2: Create Database Configuration

**Create new file:** `src/store/postgres.ts`

```typescript
import { Pool } from 'pg';

const pool = new Pool({
  connectionString: process.env.DATABASE_URL,
  ssl: process.env.NODE_ENV === 'production' ? { rejectUnauthorized: false } : false,
  max: 20,
  idleTimeoutMillis: 30000,
  connectionTimeoutMillis: 2000,
});

export default pool;
```

### Step 3.3: Create PostgreSQL Database Class

**Create new file:** `src/store/postgresdb.ts`

This file will be ~300 lines. I'll create it in the next step with all the methods matching your current FileDb interface.

### Step 3.4: Update Environment Variables

**Add to `.env.example`:**

```bash
# Database (choose one)
# Option 1: File-based (current)
# No additional config needed

# Option 2: PostgreSQL (Supabase)
DATABASE_URL=postgresql://postgres:[PASSWORD]@db.xxxxxxxxxxxxx.supabase.co:5432/postgres
```

**Add to your `.env` file:**

```bash
DATABASE_URL=postgresql://postgres:[YOUR-PASSWORD]@db.[YOUR-PROJECT-REF].supabase.co:5432/postgres
```

### Step 3.5: Update Config to Support Both Storage Types

**Update `src/config.ts`:**

```typescript
export const config = {
  // ... existing config ...
  
  storage: {
    type: process.env.STORAGE_TYPE || 'file', // 'file' or 'postgres'
  },
};
```

---

## üìù PHASE 4: Migrate Existing Data

### Step 4.1: Export Current Data

**Create migration script:** `scripts/migrate-to-postgres.ts`

```typescript
import { FileDb } from '../src/store/filedb';
import pool from '../src/store/postgres';

async function migrate() {
  console.log('Starting migration...');
  
  const fileDb = new FileDb();
  const data = fileDb.data;
  
  // Migrate users
  console.log(`Migrating ${data.users.length} users...`);
  for (const user of data.users) {
    await pool.query(
      'INSERT INTO users (tg_id, username, first_name, referred_by) VALUES ($1, $2, $3, $4) ON CONFLICT (tg_id) DO NOTHING',
      [user.tgId, user.username, user.firstName, user.referredBy]
    );
  }
  
  // Migrate watchers
  console.log(`Migrating ${data.watchers.length} watchers...`);
  for (const watcher of data.watchers) {
    await pool.query(
      'INSERT INTO watchers (tg_id, address) VALUES ($1, $2) ON CONFLICT (tg_id, address) DO NOTHING',
      [watcher.tgId, watcher.address]
    );
  }
  
  // Migrate cursors
  console.log(`Migrating ${Object.keys(data.cursors).length} cursors...`);
  for (const [address, cursor] of Object.entries(data.cursors)) {
    await pool.query(
      'INSERT INTO cursors (address, last_ts, last_tx) VALUES ($1, $2, $3) ON CONFLICT (address) DO UPDATE SET last_ts = $2, last_tx = $3',
      [address, cursor.lastTs, cursor.lastTx]
    );
  }
  
  // Migrate user settings
  console.log(`Migrating ${Object.keys(data.userSettings).length} user settings...`);
  for (const [tgId, settings] of Object.entries(data.userSettings)) {
    if (settings.minDmUsd !== undefined) {
      await pool.query(
        'INSERT INTO user_settings (tg_id, min_dm_usd) VALUES ($1, $2) ON CONFLICT (tg_id) DO UPDATE SET min_dm_usd = $2',
        [parseInt(tgId), settings.minDmUsd]
      );
    }
  }
  
  console.log('Migration complete!');
  await pool.end();
}

migrate().catch(console.error);
```

### Step 4.2: Run Migration

```bash
npx ts-node scripts/migrate-to-postgres.ts
```

---

## üìù PHASE 5: Test & Deploy

### Step 5.1: Test Locally with PostgreSQL

**Update `.env`:**

```bash
STORAGE_TYPE=postgres
DATABASE_URL=postgresql://postgres:[PASSWORD]@db.[PROJECT-REF].supabase.co:5432/postgres
```

**Run bot:**

```bash
npm run build
npm start
```

**Test commands:**

```
/start
/watch 0x9e29f8f3c63401ccc6f0ec03050494756d8157c4
/list
/settings
```

### Step 5.2: Verify Data in Supabase

1. **Go to:** Supabase Dashboard ‚Üí Table Editor
2. **Check:** watchers table has your data
3. **Check:** users table has your data

### Step 5.3: Deploy to Railway with PostgreSQL

1. **Go to:** Railway Dashboard ‚Üí Your Project
2. **Add environment variables:**
   ```
   STORAGE_TYPE=postgres
   DATABASE_URL=postgresql://postgres:[PASSWORD]@db.[PROJECT-REF].supabase.co:5432/postgres
   ```
3. **Redeploy**
4. **Monitor logs** for successful connection

---

## üìä Benefits After Migration

### Performance
- ‚úÖ **10x faster** queries (indexed lookups)
- ‚úÖ **Concurrent access** (multiple processes if needed)
- ‚úÖ **ACID transactions** (data integrity)

### Scalability
- ‚úÖ **10,000+ users** supported
- ‚úÖ **Horizontal scaling** possible
- ‚úÖ **No file I/O bottlenecks**

### Features
- ‚úÖ **Auto-cleanup** of old deduplication records
- ‚úÖ **Built-in backups** (Supabase)
- ‚úÖ **SQL analytics** (easy reporting)
- ‚úÖ **Realtime subscriptions** (future feature)

---

## üîß Troubleshooting

### Issue: Connection timeout

**Solution:**
```bash
# Check if DATABASE_URL is correct
echo $DATABASE_URL

# Test connection
psql $DATABASE_URL -c "SELECT 1"
```

### Issue: SSL error

**Solution:** Add `?sslmode=require` to connection string:
```
DATABASE_URL=postgresql://...?sslmode=require
```

### Issue: Too many connections

**Solution:** Reduce pool size in `postgres.ts`:
```typescript
max: 10, // instead of 20
```

---

## üí∞ Cost Comparison

| Users | File Storage | Supabase Free | Supabase Pro |
|-------|--------------|---------------|--------------|
| 0-500 | ‚úÖ Free | ‚úÖ Free | $25/mo |
| 500-5000 | ‚ùå Slow | ‚úÖ Free | $25/mo |
| 5000-50000 | ‚ùå Won't work | ‚ö†Ô∏è May hit limits | $25/mo |
| 50000+ | ‚ùå Won't work | ‚ùå Need Pro | $25/mo |

**Recommendation:** Start with Supabase Free, upgrade to Pro when you hit 50k users.

---

## üìÖ Migration Timeline

**When to migrate:**
- ‚úÖ **Now:** If you want better performance and scalability
- ‚úÖ **At 100 users:** Recommended for smooth experience
- ‚ö†Ô∏è **At 500 users:** File storage will start slowing down
- üö® **At 1000 users:** File storage will become problematic

**Current status:** You have <10 users, so file storage is fine for now.

**My recommendation:** Deploy to Railway with file storage first, then migrate to PostgreSQL in 1-2 weeks once you have real users.

---

## üéØ Next Steps

### Option A: Migrate Now (Recommended if you want production-ready)
1. Follow Phase 1-5 above
2. Test thoroughly
3. Deploy to Railway with PostgreSQL

### Option B: Deploy with File Storage First (Faster to market)
1. Deploy to Railway now (follow `RAILWAY_DEPLOYMENT_GUIDE.md`)
2. Get users and feedback
3. Migrate to PostgreSQL in 1-2 weeks

**I recommend Option B** - get your bot live first, then optimize!

---

## üìû Need Help?

If you want me to:
- [ ] Create the full `postgresdb.ts` implementation
- [ ] Create the migration script
- [ ] Help with Supabase setup
- [ ] Test the migration

Just let me know and I'll do it!

---

**üéâ You now have a complete PostgreSQL migration plan!**

